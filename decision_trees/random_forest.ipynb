{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Answer\n",
    "\n",
    "1.  If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why? \n",
    "\n",
    "    * Yes. Combine the models into a voting classifier. The greater the difference between the models the better it will perform. \n",
    "\n",
    "2. What is the difference between hard and soft voting classifiers?\n",
    "\n",
    "    * Hard classifier selects the class that gets the most votes. Soft classifier picks the highest class probability. Soft voting tends to            yield higher performance due to the weight it gives to confident votes.\n",
    "\n",
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "\n",
    "    * Yes, it is possible to spead up ensemble training by distributing each classifier accross multiple servers.        This is because each classifer operates in dependant of each other.\n",
    "\n",
    "4. What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "    * Becuase the OOB samples were not selected during bagging they can serve as a validation set. These samples         will provide an unbiased measure of the ensemble's performance.   \n",
    "\n",
    "5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    "\n",
    "    * Thresholds for splitting nodes are determined at random. Random thresholds will speed up training time and         decrease model variance (at the expense of bias)\n",
    "\n",
    "6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    "\n",
    "    * Increase the number of estimators, increase the learning rate, and adjust the base estimator.\n",
    "\n",
    "7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "\n",
    "    * Reduce the learning the learning rate and reduce the numnber of trees (use early stopping to find right            number of predictors)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On\n",
    "\n",
    "8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances    for training, 10,000 for validation, and 10,000 for testing). \n",
    "\n",
    "   Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. \n",
    "\n",
    "   Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. \n",
    "\n",
    "   Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# fetch and load mnist\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into test and train_val portions\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "# split train_val into seperate train and validate \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=10000, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# train a set of classifiers/predictors/estimators\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                       warm_start=False)\nTraining: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=100,\n                     n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                     warm_start=False)\nTraining: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n          verbose=0)\nTraining: MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(100,), learning_rate='constant',\n              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n              validation_fraction=0.1, verbose=False, warm_start=False)\n"
    }
   ],
   "source": [
    "# fit estimators to training data\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training:\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.9692, 0.9715, 0.8626, 0.9582]"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# access and print scores for each estimator\n",
    "scores = [estimator.score(X_val, y_val) for estimator in estimators]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VotingClassifier(estimators=[('random_forest_clf',\n                              RandomForestClassifier(bootstrap=True,\n                                                     class_weight=None,\n                                                     criterion='gini',\n                                                     max_depth=None,\n                                                     max_features='auto',\n                                                     max_leaf_nodes=None,\n                                                     min_impurity_decrease=0.0,\n                                                     min_impurity_split=None,\n                                                     min_samples_leaf=1,\n                                                     min_samples_split=2,\n                                                     min_weight_fraction_leaf=0.0,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     oob_score=False,\n                                                     random_st...\n                                            beta_2=0.999, early_stopping=False,\n                                            epsilon=1e-08,\n                                            hidden_layer_sizes=(100,),\n                                            learning_rate='constant',\n                                            learning_rate_init=0.001,\n                                            max_iter=200, momentum=0.9,\n                                            n_iter_no_change=10,\n                                            nesterovs_momentum=True,\n                                            power_t=0.5, random_state=42,\n                                            shuffle=True, solver='adam',\n                                            tol=0.0001, validation_fraction=0.1,\n                                            verbose=False, warm_start=False))],\n                 flatten_transform=True, n_jobs=None, voting='hard',\n                 weights=None)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# define estimators to vote on\n",
    "named_estimators = [('random_forest_clf', random_forest_clf), ('extra_trees_clf', extra_trees_clf), ('svm_clf', svm_clf), ('mlp_clf', mlp_clf)]\n",
    "\n",
    "# initialize voting classifier\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "\n",
    "# fit classifier to training data\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9735"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# score the voting classifier\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.9692, 0.9715, 0.8626, 0.9582]"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# access and print scores for each vote in classifier\n",
    "scores = [estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VotingClassifier(estimators=[('random_forest_clf',\n                              RandomForestClassifier(bootstrap=True,\n                                                     class_weight=None,\n                                                     criterion='gini',\n                                                     max_depth=None,\n                                                     max_features='auto',\n                                                     max_leaf_nodes=None,\n                                                     min_impurity_decrease=0.0,\n                                                     min_impurity_split=None,\n                                                     min_samples_leaf=1,\n                                                     min_samples_split=2,\n                                                     min_weight_fraction_leaf=0.0,\n                                                     n_estimators=100,\n                                                     n_jobs=None,\n                                                     oob_score=False,\n                                                     random_st...\n                                            beta_2=0.999, early_stopping=False,\n                                            epsilon=1e-08,\n                                            hidden_layer_sizes=(100,),\n                                            learning_rate='constant',\n                                            learning_rate_init=0.001,\n                                            max_iter=200, momentum=0.9,\n                                            n_iter_no_change=10,\n                                            nesterovs_momentum=True,\n                                            power_t=0.5, random_state=42,\n                                            shuffle=True, solver='adam',\n                                            tol=0.0001, validation_fraction=0.1,\n                                            verbose=False, warm_start=False))],\n                 flatten_transform=True, n_jobs=None, voting='hard',\n                 weights=None)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# svm performs poorly, remove it\n",
    "voting_clf.set_params(svm_clf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('random_forest_clf',\n  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n                         min_impurity_decrease=0.0, min_impurity_split=None,\n                         min_samples_leaf=1, min_samples_split=2,\n                         min_weight_fraction_leaf=0.0, n_estimators=100,\n                         n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                         warm_start=False)),\n ('extra_trees_clf',\n  ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                       warm_start=False)),\n ('svm_clf', None),\n ('mlp_clf',\n  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n                beta_2=0.999, early_stopping=False, epsilon=1e-08,\n                hidden_layer_sizes=(100,), learning_rate='constant',\n                learning_rate_init=0.001, max_iter=200, momentum=0.9,\n                n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n                random_state=42, shuffle=True, solver='adam', tol=0.0001,\n                validation_fraction=0.1, verbose=False, warm_start=False))]"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# note svm is in the estimator list as none\n",
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n                        min_impurity_decrease=0.0, min_impurity_split=None,\n                        min_samples_leaf=1, min_samples_split=2,\n                        min_weight_fraction_leaf=0.0, n_estimators=100,\n                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                        warm_start=False),\n ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=100,\n                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                      warm_start=False),\n LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n           multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n           verbose=0),\n MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n               hidden_layer_sizes=(100,), learning_rate='constant',\n               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n               random_state=42, shuffle=True, solver='adam', tol=0.0001,\n               validation_fraction=0.1, verbose=False, warm_start=False)]"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# trained svm is still in estimator list\n",
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the voting classfier again or delete svm from estimators_\n",
    "del voting_clf.estimators_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9735"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# evaluate voting classifier without svm\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.967"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# set voting to soft by overwriting voting attribute\n",
    "voting_clf.voting = 'soft'\n",
    "\n",
    "# score with soft voting\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9702"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# hard voting had a better score, reset attribute\n",
    "voting_clf.voting = 'hard'\n",
    "\n",
    "# now score voting classifier on the test set\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.9645, 0.9691, 0.9581]"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# score each vote in classifer on test set\n",
    "scores = [estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]\n",
    "scores\n",
    "\n",
    "# in this case the voting classifier only slightly improved model accuracy. is it worth it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the      resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the      target is the image's class. \n",
    "\n",
    "   Train a classifier on this new training set.\n",
    "\n",
    "   Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the        ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the           blender to get the ensemble's predictions. \n",
    "   \n",
    "   How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[5., 5., 5., 5.],\n       [8., 8., 8., 8.],\n       [2., 2., 2., 2.],\n       ...,\n       [7., 7., 7., 7.],\n       [6., 6., 6., 6.],\n       [7., 7., 7., 7.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# make array to hold new training set\n",
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "# make predictions and assign them to new training set\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)\n",
    "\n",
    "X_val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=None, oob_score=True, random_state=42, verbose=0,\n                       warm_start=False)"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# initilaize random forest blender\n",
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "# train blender on new training set\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9695"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# evaluate blender score\n",
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run blender on test set\n",
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)\n",
    "\n",
    "# make predictions\n",
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.966"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# evaluate blender score on test set \n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# in this case the blender does not perform as well as the voting classifier or the extra trees random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}